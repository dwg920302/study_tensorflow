# 실습
# 위의 코드에서 lr을 수정하고, epoch를 2000이 아니라 100번 이하로 줄이기
# 결과치는 step <= 100, w = 1.9999, b = 0.9999

import tensorflow as tf

from icecream import ic

# y = wx + b

# tf.set_random_seed(74)

x_train = tf.compat.v1.placeholder(tf.float32, shape=[None])
y_train = tf.compat.v1.placeholder(tf.float32, shape=[None])

x_test = tf.compat.v1.placeholder(tf.float32, shape=[None])

w = tf.Variable(tf.random_normal([1]), dtype=tf.float32)    #, name='test)
b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)

hypothesis = x_train * w + b
# f(x) = wx + b

loss = tf.reduce_mean(tf.square(hypothesis - y_train))  # mse

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.17325)

# optimizer = tf.train.AdamOptimizer(learning_rate=1/10)

train = optimizer.minimize(loss)

session = tf.Session()
session.run(tf.global_variables_initializer())  # 초기화

epochs = 101
# 이게 100 이하로 됨??? ㄷㄷ 300도 간신히 되는데

for step in range(epochs):
    _, loss_val, w_val, b_val= session.run([train, loss, w, b], feed_dict={x_train:[1, 2, 3], y_train:[3, 5, 7]})
    print(step, loss_val, w_val, b_val)
    # print(step, session.run(loss), session.run(w), session.run(b))
    
# Predict 추가
x_test_set = [[4], [5, 6], [6, 7, 8]]

pred_hypothesis = x_test * w_val + b_val

for x_test_value in x_test_set:
    pred = session.run(pred_hypothesis, feed_dict={x_test:x_test_value})

    test = optimizer.minimize(loss)

    print(pred)

'''
lr = 1/7 (0.142857...)

91 5.3445474e-05 [1.9918007] [1.0186388]
92 4.983839e-05 [1.9920825] [1.0179988]
93 4.6474426e-05 [1.9923542] [1.0173806]
94 4.3335993e-05 [1.9926169] [1.0167838]
95 4.041113e-05 [1.9928702] [1.0162073]
96 3.7684003e-05 [1.9931153] [1.0156509]
97 3.5140252e-05 [1.9933516] [1.0151134]
98 3.2766617e-05 [1.9935799] [1.0145943]
99 3.0555897e-05 [1.9938005] [1.0140932]
100 2.8491697e-05 [1.9940132] [1.013609]
8.989662

lr = 1/6 (0.166666...)
91 4.5890174e-05 [1.992447] [1.0171684]
92 4.2284304e-05 [1.9927504] [1.0164809]
93 3.896418e-05 [1.9930402] [1.0158203]
94 3.590617e-05 [1.9933197] [1.0151867]
95 3.3083692e-05 [1.9935867] [1.0145779]
96 3.0485973e-05 [1.9938444] [1.0139942]
97 2.8094748e-05 [1.9940903] [1.0134332]
98 2.5885796e-05 [1.9943275] [1.0128952]
99 2.3854276e-05 [1.9945546] [1.0123785]
100 2.1982183e-05 [1.9947729] [1.0118825]
8.990974

lr = 1/5 (0.2) -> 값이 산으로 갔음
91 2.0104227e+17 [-2.123629e+08] [-93418800.]
92 2.9853916e+17 [2.587829e+08] [1.1383906e+08]
93 4.4331776e+17 [-3.1534976e+08] [-1.387229e+08]
94 6.5830785e+17 [3.8428147e+08] [1.690461e+08]
95 9.7755895e+17 [-4.682809e+08] [-2.0599755e+08]
96 1.4516334e+18 [5.7064154e+08] [2.5102626e+08]
97 2.1556133e+18 [-6.9537715e+08] [-3.058976e+08]
98 3.2009942e+18 [8.473784e+08] [3.727632e+08]
99 4.753339e+18 [-1.0326053e+09] [-4.542448e+08]
100 7.0585057e+18 [1.2583208e+09] [5.5353754e+08]
5586820600.0

lr = 1/4 (0.25) -> 값이 산에서 우주로 갔음, 더 이상 늘려볼 필요가 없음
91 inf [-2.7667825e+23] [-1.2171123e+23]
92 inf [4.9061566e+23] [2.1582265e+23]
93 inf [-8.6997684e+23] [-3.8270433e+23]
94 inf [1.5426735e+24] [6.786247e+23]
95 inf [-2.7355227e+24] [-1.2033611e+24]
96 inf [4.8507247e+24] [2.1338423e+24]
97 inf [-8.601476e+24] [-3.7838042e+24]
98 inf [1.5252438e+25] [6.709573e+24]
99 inf [-2.704616e+25] [-1.1897652e+25]
100 inf [4.79592e+25] [2.1097336e+25]
inf

lr = 0.18
91 29.45785 [-0.09756994] [0.06364167]
92 29.265474 [4.100526] [1.9109812]
93 29.074354 [-0.08426428] [0.07064927]
94 28.884481 [4.086433] [1.9058859]
95 28.695852 [-0.07101202] [0.07753527]
96 28.508444 [4.072463] [1.9007515]
97 28.32226 [-0.05781651] [0.08430743]
98 28.137308 [4.058614] [1.8955846]
99 27.953547 [-0.04467821] [0.09097195]
100 27.770987 [4.044882] [1.8903904]
18.069918

lr = 0.17
91 2.2502616e-05 [2.0052373] [0.9879722]
92 2.06979e-05 [2.0051064] [0.98850024]
93 1.9040208e-05 [2.0048242] [0.9889378]
94 1.7513916e-05 [2.004692] [0.9894185]
95 1.611068e-05 [2.0044427] [0.9898256]
96 1.48195795e-05 [2.004312] [0.9902638]
97 1.3632292e-05 [2.0040908] [0.9906419]
98 1.2541007e-05 [2.0039635] [0.9910419]
99 1.15363255e-05 [2.003766] [0.99139243]
100 1.0612442e-05 [2.0036435] [0.99175805]
9.006332

lr = 0.171
91 2.8265049e-08 [1.9999458] [0.9999643]
92 2.2721622e-08 [2.0000567] [1.0000136]
93 1.8304206e-08 [1.9999568] [0.99997014]
94 1.4792174e-08 [2.0000463] [1.0000099]
95 1.1936588e-08 [1.9999655] [0.99997485]
96 9.6508e-09 [2.000038] [1.000007]
97 7.815667e-09 [1.9999726] [0.9999787]
98 6.29772e-09 [2.000031] [1.0000048]
99 5.0969597e-09 [1.9999783] [0.99998194]
100 4.0843324e-09 [2.0000253] [1.000003]
9.000104

lr = 0.17115
91 1.945777e-07 [2.0004852] [0.99888134]
92 1.7887254e-07 [2.0004761] [0.9989321]
93 1.6448068e-07 [2.0004466] [0.9989717]
94 1.513378e-07 [2.000437] [0.99901795]
95 1.3897268e-07 [2.0004113] [0.9990549]
96 1.2782685e-07 [2.0004013] [0.9990968]
97 1.1741404e-07 [2.0003786] [0.99913126]
98 1.08013104e-07 [2.0003686] [0.99916947]
99 9.9309695e-08 [2.0003486] [0.9992015]
100 9.124394e-08 [2.0003386] [0.9992362]
9.00059

lr = 0.17325
91 1.7371718e-05 [1.9965934] [1.0041184]
92 1.4999976e-05 [1.9992479] [1.0050522]
93 1.2965421e-05 [1.9969629] [1.0038228]
94 1.121888e-05 [1.9992248] [1.0046029]
95 9.718596e-06 [1.9972885] [1.0035452]
96 8.428285e-06 [1.9992162] [1.0041959]
97 7.317787e-06 [1.9975759] [1.0032852]
98 6.3611546e-06 [1.9992191] [1.0038267]
99 5.5363366e-06 [1.9978299] [1.003042]
100 4.8251554e-06 [1.9992309] [1.0034918]
[9.000415]
[10.999646 12.998877]
[12.998877 14.998107 16.99734 ]
'''